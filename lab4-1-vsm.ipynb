{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *J*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Maxime Lucas Lanvin*\n",
    "* *Victor Salvia*\n",
    "* *Erik Axel Wilhelm Sjöberg*\n",
    "\n",
    "---\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.util import ngrams \n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from numpy.linalg import norm\n",
    "import copy\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing\n",
    "\n",
    "Pre-process the corpus to create bag-of-words representations of each document. You are free\n",
    "to proceed as you wish.\n",
    "\n",
    "1. Explain which ones you implemented and why.\n",
    "2. Print the terms in the pre-processed description of the IX class in alphabetical order\n",
    "\n",
    "#### Removing special characters, lemmazation and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the tokenizer, the stemmer and the lemmatizer.\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True,preserve_case=False)\n",
    "ps = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list containing all of the special chars. This list is then added to the stopwords and together they form\n",
    "# the ignored words. These words will be removed from the corpus. \n",
    "specialchar = ['.', ',', '(', ')', '&', ':', '/','-','\"',';','', ' ', '..', '...',\"'\",'%']\n",
    "ignored_words = set(list(stopwords) + specialchar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks whether or not there is a digit in a string.\n",
    "def NoNumbers(s):\n",
    "    return not any(char.isdigit() for char in s)\n",
    "\n",
    "# Stemms a given string\n",
    "def stemmer(s):\n",
    "    word_tokens = tknzr.tokenize(s)\n",
    "    temp_list = [ps.stem(w) for w in word_tokens if not w in ignored_words] \n",
    "    return [w for w in temp_list if NoNumbers(w)]\n",
    "\n",
    "# Lemmatizes a given string\n",
    "def lemmazation(s):\n",
    "    word_tokens = tknzr.tokenize(s)\n",
    "    temp_list = [lemmatizer.lemmatize(w) for w in word_tokens if not w in ignored_words]\n",
    "    return [w for w in temp_list if NoNumbers(w)]\n",
    "\n",
    "def lem_n_stem(s):\n",
    "    word_tokens = tknzr.tokenize(s)\n",
    "    temp_list = [ps.stem(lemmatizer.lemmatize(w)) for w in word_tokens if not w in ignored_words]\n",
    "    return [w for w in temp_list if NoNumbers(w)]    \n",
    "\n",
    "# Helper function for the tokenize_1gram.\n",
    "# tokenzie a given string, either stem or Lemmatise the words and removes the ignored words for a 1 gram\n",
    "def tokenize_1gram(l,lem,stemlem):\n",
    "    courses_loc = copy.deepcopy(l)\n",
    "    for i in courses_loc:\n",
    "        description = i['description']\n",
    "        if lem == True:\n",
    "            i['description'] = lemmazation(description)\n",
    "            if stemlem == False:\n",
    "                i['description'] = lemmazation(description)\n",
    "            else:\n",
    "                i['description'] = lem_n_stem(description)\n",
    "        else:\n",
    "            i['description'] = stemmer(description)  \n",
    "    return courses_loc\n",
    "\n",
    "# Description: Tokenzie a given string, either stem or Lemmatise the words and removes the ignored words for a 1 gram.\n",
    "# After this step n-grams are created over the cleaned string //\n",
    "\n",
    "# @ l: Indicats the level of the n-gram we want returned over the string l. Default is 1.\n",
    "# @ lem: boolean exression determining whether or not to use stemming or lemmazation. Default is lemmazation\n",
    "# @ stemlem: boolean expression determining whether or not to use both stemming and lemmazation\n",
    "def tokenize_ngram(l,n=1,lem=True,stemlem=False):\n",
    "    if n ==1:\n",
    "        return tokenize_1gram(l,lem,stemlem)  \n",
    "    courses_loc = copy.deepcopy(l)\n",
    "    for i in courses_loc:\n",
    "        description = i['description']\n",
    "        sentences = description.split('.')\n",
    "        grams = []\n",
    "        for s in sentences:\n",
    "            if lem == True:\n",
    "                if stemlem == False:\n",
    "                    tokens = lemmazation(s)\n",
    "                else:\n",
    "                    tokens = lem_n_stem(s)\n",
    "            else:\n",
    "                tokens = stemmer(s)\n",
    "            grams = grams + list(ngrams(tokens,n))\n",
    "        i['description'] = grams\n",
    "    return courses_loc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenzing and Lemmatizing the corpus, for 1-grams, 2-grams and 3-grams\n",
    "lemmatized_1gram = tokenize_ngram(courses,1)\n",
    "lemmatized_2gram = tokenize_ngram(courses,2)\n",
    "lemmatized_3gram = tokenize_ngram(courses,3)\n",
    "\n",
    "# Tokenzing and Stemming the corpus, for 1-grams, 2-grams and 3-grams\n",
    "stemmed_1gram = tokenize_ngram(courses,1,lem=False)\n",
    "stemmed_2gram = tokenize_ngram(courses,2,lem=False)\n",
    "stemmed_3gram = tokenize_ngram(courses,3,lem=False)\n",
    "\n",
    "# Tokenzing, Lemmatizing & Stemming the corpus, for 1-grams, 2-grams and 3-grams\n",
    "lem_and_stem_1gram = tokenize_ngram(courses,1,stemlem=True)\n",
    "lem_and_stem_2gram = tokenize_ngram(courses,2,stemlem=True)\n",
    "lem_and_stem_3gram = tokenize_ngram(courses,3,stemlem=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we have removed all of the stopwords and some special characters defined above, (We used the stopwords provided by us in the handout). Moreover we collected all the bigrams and trigrams from every SENTENCE. This means that the description was split into its sentences and from these the bigrams and trigrams were collected. This was done because the words after the punctuation are assumed to not be connected with the words before punctuation. Words containing numbers were also removed from the each Corpus. These words were often things like courseIDs, percentage, numbers before time (e.g 1.5 hours), etc. In the end of this step, we had 9 Corpora in total. These are the ones listed directly above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing lesser and very common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_frequent_infrequent_words(d,lower_limit=5,higher_limit=500):\n",
    "    global_dictionary, dictionary_mapping = get_dictionary(d)\n",
    "    counts = dict(Counter(global_dictionary))\n",
    "    low_freq_grams = dict((k, v) for (k,v) in counts.items() if v < lower_limit)\n",
    "    high_freq_grams = dict((k, v) for (k,v) in counts.items() if v > higher_limit)\n",
    "    \n",
    "    cleaned_dict = copy.deepcopy(d)\n",
    "    \n",
    "    # Would be nice to use a helper here\n",
    "    for k,v in low_freq_grams.items():\n",
    "        temp_list = dictionary_mapping[k]\n",
    "        for u in temp_list:\n",
    "            cleaned_dict[u]['description'].remove(k)\n",
    "    for k,v in high_freq_grams.items():\n",
    "        temp_list = dictionary_mapping[k]\n",
    "        for u in temp_list:\n",
    "            cleaned_dict[u]['description'].remove(k)\n",
    "    return cleaned_dict\n",
    "    \n",
    "def get_dictionary(d):\n",
    "    global_dictionary = []\n",
    "    dictionary_mapping = {}\n",
    "    for i in range(0,len(d)):\n",
    "        temp_list = d[i]\n",
    "        global_dictionary = global_dictionary + temp_list['description']\n",
    "        for w in temp_list['description']:\n",
    "            if w in dictionary_mapping:\n",
    "                dictionary_mapping[w].append(i)\n",
    "            else:\n",
    "                dictionary_mapping[w] = [i]\n",
    "    return global_dictionary, dictionary_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dictionary, dictionary_mapping = get_dictionary(lemmatized_1gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('student', 2029),\n",
       " ('method', 1765),\n",
       " ('learning', 1472),\n",
       " ('system', 1063),\n",
       " ('content', 917),\n",
       " ('model', 788),\n",
       " ('design', 787),\n",
       " ('course', 759),\n",
       " ('analysis', 727),\n",
       " ('basic', 702)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of very common words\n",
    "Counter(global_dictionary).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mandelbrot', 1),\n",
       " ('matplotlib', 1),\n",
       " ('lapack', 1),\n",
       " ('calculati', 1),\n",
       " ('blokesch', 1),\n",
       " ('fluorescently', 1),\n",
       " ('bacterium', 1),\n",
       " ('unknown', 1),\n",
       " ('microbetracker', 1),\n",
       " ('artifical', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of lesser common words\n",
    "n=10\n",
    "Counter(global_dictionary).most_common()[:-n-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dict(Counter(global_dictionary))\n",
    "low_freq_words = dict((k, v) for (k,v) in counts.items() if v < 3)\n",
    "high_freq_words = dict((k, v) for (k,v) in counts.items() if v > 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words that are removed due do low frequency in lemmatized_1gram: 10703\n",
      "The number of words that are removed due do high frequency in lemmatized_1gram: 25\n"
     ]
    }
   ],
   "source": [
    "# For Lemmatizing\n",
    "print('The number of words that are removed due do low frequency in lemmatized_1gram: ' + str(len(low_freq_words)))\n",
    "print('The number of words that are removed due do high frequency in lemmatized_1gram: ' + str(len(high_freq_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_1gram = remove_frequent_infrequent_words(lemmatized_1gram,lower_limit=3,higher_limit=500)\n",
    "lemmatized_2gram = remove_frequent_infrequent_words(lemmatized_2gram,lower_limit=3,higher_limit=400)\n",
    "lemmatized_3gram = remove_frequent_infrequent_words(lemmatized_3gram,lower_limit=2,higher_limit=400)\n",
    "\n",
    "stemmed_1gram = remove_frequent_infrequent_words(stemmed_1gram,lower_limit=3,higher_limit=500)\n",
    "stemmed_2gram = remove_frequent_infrequent_words(stemmed_2gram,lower_limit=3,higher_limit=400)\n",
    "stemmed_3gram = remove_frequent_infrequent_words(stemmed_3gram,lower_limit=2,higher_limit=400)\n",
    "\n",
    "lem_and_stem_1gram = remove_frequent_infrequent_words(lem_and_stem_1gram,lower_limit=3,higher_limit=500)\n",
    "lem_and_stem_2gram = remove_frequent_infrequent_words(lem_and_stem_2gram,lower_limit=3,higher_limit=400)\n",
    "lem_and_stem_3gram = remove_frequent_infrequent_words(lem_and_stem_3gram,lower_limit=2,higher_limit=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we have removed the very common and lesser common words from the Corpora. For 1-grams, words that occur less than 3 times are removed. 2-grams that occur less than 3 times and 3-grams that occur only once are removed. The reason why we have a lower limit for the 3-grams is because if they occur several times it is more deliberate than a single word or a bigram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index of Internet Analytics course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_name_to_id = {}\n",
    "for i in range(len(courses)):\n",
    "    course_name_to_id[courses[i]['name']] = i\n",
    "course_id_to_name = dict((v,k) for k,v in course_name_to_id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding COM-308's place in the list. \n",
    "course_name_to_id['Internet analytics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------- 1 grams -------------------------------------------------\n",
      "['acquired', 'ad', 'ad', 'algebra', 'algebra', 'algorithm', 'algorithm', 'analytics', 'analytics', 'auction', 'auction', 'balance', 'based', 'based', 'cathedra', 'chain', 'class', 'class', 'class', 'cloud', 'clustering', 'clustering', 'collection', 'combination', 'communication', 'community', 'community', 'computing', 'computing', 'concrete', 'coverage', 'current', 'data', 'data', 'data', 'data', 'data', 'data', 'datasets', 'datasets', 'decade', 'dedicated', 'designed', 'detection', 'detection', 'dimensionality', 'draw', 'e-commerce', 'e-commerce', 'effectiveness', 'efficiency', 'exam', 'expected', 'explore', 'explore', 'explore', 'explore', 'explores', 'field', 'final', 'foundational', 'framework', 'function', 'fundamental', 'good', 'graph', 'graph', 'hadoop', 'hadoop', 'hands-on', 'homework', 'homework', 'important', 'information', 'information', 'infrastructure', 'inspired', 'internet', 'internet', 'java', 'key', 'knowledge', 'lab', 'lab', 'lab', 'laboratory', 'large-scale', 'large-scale', 'large-scale', 'linear', 'linear', 'machine', 'machine', 'main', 'map-reduce', 'markov', 'medium', 'midterm', 'mining', 'mining', 'mining', 'modeling', 'network', 'networking', 'networking', 'networking', 'number', 'number', 'online', 'online', 'online', 'online', 'online', 'past', 'practical', 'practice', 'problem', 'problem', 'provide', 'question', 'real', 'real-world', 'real-world', 'real-world', 'real-world', 'recommended', 'recommender', 'recommender', 'reduction', 'related', 'required', 'retrieval', 'retrieval', 'search', 'search', 'seek', 'service', 'service', 'service', 'session', 'session', 'social', 'social', 'social', 'social', 'social', 'spark', 'specifically', 'start', 'statistic', 'stochastic', 'stream', 'stream', 'structure', 'technique', 'theoretical', 'theory', 'topic', 'topic', 'typical', 'ubiquitous', 'user', 'weekly', 'world']\n",
      "\n",
      "--------------------------------------------------- 2 grams -------------------------------------------------\n",
      "[('activity', 'lecture'), ('algorithm', 'data'), ('basic', 'linear'), ('basic', 'material'), ('basic', 'model'), ('community', 'detection'), ('community', 'detection'), ('concept', 'start'), ('content', 'class'), ('course', 'basic'), ('course', 'stochastic'), ('data', 'mining'), ('data', 'mining'), ('data', 'mining'), ('data', 'structure'), ('dimensionality', 'reduction'), ('expected', 'student'), ('final', 'exam'), ('fundamental', 'concept'), ('graph', 'theory'), ('important', 'concept'), ('information', 'retrieval'), ('keywords', 'data'), ('knowledge', 'acquired'), ('lab', 'session'), ('laboratory', 'session'), ('learning', 'social'), ('learning', 'technique'), ('lecture', 'homework'), ('linear', 'algebra'), ('linear', 'algebra'), ('machine', 'learning'), ('machine', 'learning'), ('markov', 'chain'), ('method', 'cathedra'), ('method', 'project'), ('midterm', 'final'), ('model', 'communication'), ('modeling', 'analysis'), ('practical', 'question'), ('prerequisite', 'required'), ('problem', 'teaching'), ('project', 'midterm'), ('real', 'world'), ('real-world', 'application'), ('real-world', 'problem'), ('recommended', 'course'), ('recommender', 'system'), ('recommender', 'system'), ('related', 'field'), ('required', 'course'), ('session', 'expected'), ('social', 'medium'), ('social', 'networking'), ('social', 'networking'), ('social', 'networking'), ('stochastic', 'model'), ('structure', 'important'), ('student', 'activity'), ('student', 'explore'), ('topic', 'model'), ('topic', 'model')]\n",
      "\n",
      "--------------------------------------------------- 3 grams -------------------------------------------------\n",
      "[('algorithm', 'data', 'structure'), ('assessment', 'method', 'project'), ('basic', 'linear', 'algebra'), ('clustering', 'community', 'detection'), ('clustering', 'community', 'detection'), ('communication', 'recommended', 'course'), ('course', 'basic', 'linear'), ('course', 'stochastic', 'model'), ('data', 'mining', 'machine'), ('end', 'student', 'explore'), ('expected', 'student', 'activity'), ('important', 'concept', 'start'), ('learning', 'prerequisite', 'required'), ('machine', 'learning', 'technique'), ('method', 'cathedra', 'homework'), ('midterm', 'final', 'exam'), ('mining', 'machine', 'learning'), ('prerequisite', 'required', 'course'), ('problem', 'teaching', 'method'), ('recommended', 'course', 'basic'), ('recommender', 'system', 'clustering'), ('recommender', 'system', 'clustering'), ('required', 'course', 'stochastic'), ('session', 'expected', 'student'), ('social', 'networking', 'e-commerce'), ('social', 'networking', 'e-commerce'), ('stochastic', 'model', 'communication'), ('structure', 'important', 'concept'), ('student', 'activity', 'lecture'), ('system', 'clustering', 'community'), ('system', 'clustering', 'community'), ('teaching', 'method', 'cathedra')]\n"
     ]
    }
   ],
   "source": [
    "ix_id = 43\n",
    "temp_1 = lemmatized_1gram[ix_id]['description'].copy()\n",
    "temp_2 = lemmatized_2gram[ix_id]['description'].copy()\n",
    "temp_3 = lemmatized_3gram[ix_id]['description'].copy()\n",
    "temp_1.sort(), temp_2.sort(), temp_3.sort()\n",
    "print('--------------------------------------------------- 1 grams -------------------------------------------------')\n",
    "print(temp_1), \n",
    "print('\\n--------------------------------------------------- 2 grams -------------------------------------------------')\n",
    "print(temp_2)\n",
    "print('\\n--------------------------------------------------- 3 grams -------------------------------------------------')\n",
    "print(temp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------- 1 grams -------------------------------------------------\n",
      "['acquir', 'ad', 'ad', 'algebra', 'algebra', 'algorithm', 'algorithm', 'analyt', 'analyt', 'auction', 'auction', 'balanc', 'base', 'base', 'cathedra', 'chain', 'class', 'class', 'class', 'cloud', 'cluster', 'cluster', 'collect', 'combin', 'commun', 'commun', 'commun', 'comput', 'comput', 'concret', 'coverag', 'current', 'data', 'data', 'data', 'data', 'data', 'data', 'dataset', 'dataset', 'decad', 'dedic', 'detect', 'detect', 'dimension', 'draw', 'e-commerc', 'e-commerc', 'effect', 'effici', 'exam', 'expect', 'explor', 'explor', 'explor', 'explor', 'explor', 'field', 'final', 'foundat', 'framework', 'function', 'fundament', 'good', 'graph', 'graph', 'hadoop', 'hadoop', 'hands-on', 'homework', 'homework', 'import', 'inform', 'inform', 'infrastructur', 'inspir', 'internet', 'internet', 'java', 'key', 'knowledg', 'lab', 'lab', 'lab', 'laboratori', 'large-scal', 'large-scal', 'large-scal', 'linear', 'linear', 'machin', 'machin', 'main', 'map-reduc', 'markov', 'media', 'midterm', 'mine', 'mine', 'mine', 'network', 'network', 'network', 'network', 'number', 'number', 'onlin', 'onlin', 'onlin', 'onlin', 'onlin', 'past', 'practic', 'practic', 'problem', 'problem', 'provid', 'question', 'real', 'real-world', 'real-world', 'real-world', 'real-world', 'recommend', 'recommend', 'recommend', 'reduct', 'relat', 'requir', 'retriev', 'retriev', 'search', 'search', 'seek', 'servic', 'servic', 'servic', 'session', 'session', 'social', 'social', 'social', 'social', 'social', 'spark', 'specif', 'start', 'statist', 'stochast', 'stream', 'stream', 'techniqu', 'theoret', 'theori', 'topic', 'topic', 'typic', 'ubiquit', 'user', 'weekli', 'world']\n",
      "\n",
      "--------------------------------------------------- 2 grams -------------------------------------------------\n",
      "[('activ', 'lectur'), ('algorithm', 'data'), ('algorithm', 'statist'), ('basic', 'linear'), ('basic', 'materi'), ('basic', 'model'), ('commun', 'detect'), ('commun', 'detect'), ('concept', 'start'), ('content', 'class'), ('cours', 'basic'), ('cours', 'stochast'), ('data', 'mine'), ('data', 'mine'), ('data', 'mine'), ('data', 'structur'), ('design', 'explor'), ('dimension', 'reduct'), ('expect', 'student'), ('final', 'exam'), ('fundament', 'concept'), ('graph', 'theori'), ('import', 'concept'), ('inform', 'retriev'), ('keyword', 'data'), ('knowledg', 'acquir'), ('lab', 'session'), ('laboratori', 'session'), ('learn', 'social'), ('learn', 'techniqu'), ('lectur', 'homework'), ('linear', 'algebra'), ('linear', 'algebra'), ('machin', 'learn'), ('machin', 'learn'), ('markov', 'chain'), ('method', 'cathedra'), ('method', 'project'), ('midterm', 'final'), ('model', 'analysi'), ('model', 'commun'), ('network', 'recommend'), ('practic', 'question'), ('prerequisit', 'requir'), ('problem', 'teach'), ('project', 'midterm'), ('real', 'world'), ('real-world', 'applic'), ('real-world', 'problem'), ('recommend', 'cours'), ('recommend', 'system'), ('recommend', 'system'), ('relat', 'field'), ('requir', 'cours'), ('session', 'expect'), ('social', 'media'), ('social', 'network'), ('social', 'network'), ('social', 'network'), ('stochast', 'model'), ('structur', 'import'), ('student', 'activ'), ('student', 'explor'), ('system', 'cluster'), ('system', 'cluster'), ('topic', 'model'), ('topic', 'model')]\n",
      "\n",
      "--------------------------------------------------- 3 grams -------------------------------------------------\n",
      "[('algorithm', 'data', 'structur'), ('assess', 'method', 'project'), ('basic', 'linear', 'algebra'), ('cluster', 'commun', 'detect'), ('cluster', 'commun', 'detect'), ('commun', 'recommend', 'cours'), ('cours', 'basic', 'linear'), ('cours', 'stochast', 'model'), ('data', 'mine', 'machin'), ('end', 'student', 'explor'), ('expect', 'student', 'activ'), ('import', 'concept', 'start'), ('learn', 'prerequisit', 'requir'), ('machin', 'learn', 'techniqu'), ('method', 'cathedra', 'homework'), ('midterm', 'final', 'exam'), ('mine', 'machin', 'learn'), ('prerequisit', 'requir', 'cours'), ('problem', 'teach', 'method'), ('recommend', 'cours', 'basic'), ('recommend', 'system', 'cluster'), ('recommend', 'system', 'cluster'), ('requir', 'cours', 'stochast'), ('session', 'expect', 'student'), ('social', 'network', 'e-commerc'), ('social', 'network', 'e-commerc'), ('stochast', 'model', 'commun'), ('structur', 'import', 'concept'), ('student', 'activ', 'lectur'), ('system', 'cluster', 'commun'), ('system', 'cluster', 'commun'), ('teach', 'method', 'cathedra')]\n"
     ]
    }
   ],
   "source": [
    "ix_id = 43\n",
    "temp_1 = stemmed_1gram[ix_id]['description'].copy()\n",
    "temp_2 = stemmed_2gram[ix_id]['description'].copy()\n",
    "temp_3 = stemmed_3gram[ix_id]['description'].copy()\n",
    "temp_1.sort(), temp_2.sort(), temp_3.sort()\n",
    "print('--------------------------------------------------- 1 grams -------------------------------------------------')\n",
    "print(temp_1), \n",
    "print('\\n--------------------------------------------------- 2 grams -------------------------------------------------')\n",
    "print(temp_2)\n",
    "print('\\n--------------------------------------------------- 3 grams -------------------------------------------------')\n",
    "print(temp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming & Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------- 1 grams -------------------------------------------------\n",
      "['acquir', 'ad', 'ad', 'algebra', 'algebra', 'algorithm', 'algorithm', 'analyt', 'analyt', 'auction', 'auction', 'balanc', 'base', 'base', 'cathedra', 'chain', 'class', 'class', 'class', 'cloud', 'cluster', 'cluster', 'collect', 'combin', 'commun', 'commun', 'commun', 'comput', 'comput', 'concret', 'coverag', 'current', 'data', 'data', 'data', 'data', 'data', 'data', 'dataset', 'dataset', 'decad', 'dedic', 'detect', 'detect', 'dimension', 'draw', 'e-commerc', 'e-commerc', 'effect', 'effici', 'exam', 'expect', 'explor', 'explor', 'explor', 'explor', 'explor', 'field', 'final', 'foundat', 'framework', 'function', 'fundament', 'good', 'graph', 'graph', 'hadoop', 'hadoop', 'hands-on', 'homework', 'homework', 'import', 'inform', 'inform', 'infrastructur', 'inspir', 'internet', 'internet', 'java', 'key', 'knowledg', 'lab', 'lab', 'lab', 'laboratori', 'large-scal', 'large-scal', 'large-scal', 'linear', 'linear', 'machin', 'machin', 'main', 'map-reduc', 'markov', 'medium', 'midterm', 'mine', 'mine', 'mine', 'network', 'network', 'network', 'network', 'number', 'number', 'onlin', 'onlin', 'onlin', 'onlin', 'onlin', 'past', 'practic', 'practic', 'problem', 'problem', 'provid', 'question', 'real', 'real-world', 'real-world', 'real-world', 'real-world', 'recommend', 'recommend', 'recommend', 'reduct', 'relat', 'requir', 'retriev', 'retriev', 'search', 'search', 'seek', 'servic', 'servic', 'servic', 'session', 'session', 'social', 'social', 'social', 'social', 'social', 'spark', 'specif', 'start', 'statist', 'stochast', 'stream', 'stream', 'techniqu', 'theoret', 'theori', 'topic', 'topic', 'typic', 'ubiquit', 'user', 'weekli', 'world']\n",
      "\n",
      "--------------------------------------------------- 2 grams -------------------------------------------------\n",
      "[('activ', 'lectur'), ('algorithm', 'data'), ('algorithm', 'statist'), ('basic', 'linear'), ('basic', 'materi'), ('basic', 'model'), ('commun', 'detect'), ('commun', 'detect'), ('concept', 'start'), ('content', 'class'), ('cours', 'basic'), ('cours', 'stochast'), ('data', 'mine'), ('data', 'mine'), ('data', 'mine'), ('data', 'structur'), ('design', 'explor'), ('dimension', 'reduct'), ('expect', 'student'), ('final', 'exam'), ('fundament', 'concept'), ('graph', 'theori'), ('import', 'concept'), ('inform', 'retriev'), ('keyword', 'data'), ('knowledg', 'acquir'), ('lab', 'session'), ('laboratori', 'session'), ('learn', 'social'), ('learn', 'techniqu'), ('lectur', 'homework'), ('linear', 'algebra'), ('linear', 'algebra'), ('machin', 'learn'), ('machin', 'learn'), ('markov', 'chain'), ('method', 'cathedra'), ('method', 'project'), ('midterm', 'final'), ('model', 'analysi'), ('model', 'commun'), ('network', 'recommend'), ('practic', 'question'), ('prerequisit', 'requir'), ('problem', 'teach'), ('project', 'midterm'), ('real', 'world'), ('real-world', 'applic'), ('real-world', 'problem'), ('recommend', 'cours'), ('recommend', 'system'), ('recommend', 'system'), ('relat', 'field'), ('requir', 'cours'), ('session', 'expect'), ('social', 'medium'), ('social', 'network'), ('social', 'network'), ('social', 'network'), ('stochast', 'model'), ('structur', 'import'), ('student', 'activ'), ('student', 'explor'), ('system', 'cluster'), ('system', 'cluster'), ('topic', 'model'), ('topic', 'model')]\n",
      "\n",
      "--------------------------------------------------- 3 grams -------------------------------------------------\n",
      "[('algorithm', 'data', 'structur'), ('assess', 'method', 'project'), ('basic', 'linear', 'algebra'), ('cluster', 'commun', 'detect'), ('cluster', 'commun', 'detect'), ('commun', 'recommend', 'cours'), ('cours', 'basic', 'linear'), ('cours', 'stochast', 'model'), ('data', 'mine', 'machin'), ('end', 'student', 'explor'), ('expect', 'student', 'activ'), ('import', 'concept', 'start'), ('learn', 'prerequisit', 'requir'), ('machin', 'learn', 'techniqu'), ('method', 'cathedra', 'homework'), ('midterm', 'final', 'exam'), ('mine', 'machin', 'learn'), ('prerequisit', 'requir', 'cours'), ('problem', 'teach', 'method'), ('recommend', 'cours', 'basic'), ('recommend', 'system', 'cluster'), ('recommend', 'system', 'cluster'), ('requir', 'cours', 'stochast'), ('session', 'expect', 'student'), ('social', 'network', 'e-commerc'), ('social', 'network', 'e-commerc'), ('stochast', 'model', 'commun'), ('structur', 'import', 'concept'), ('student', 'activ', 'lectur'), ('system', 'cluster', 'commun'), ('system', 'cluster', 'commun'), ('teach', 'method', 'cathedra')]\n"
     ]
    }
   ],
   "source": [
    "ix_id = 43\n",
    "temp_1 = lem_and_stem_1gram[ix_id]['description'].copy()\n",
    "temp_2 = lem_and_stem_2gram[ix_id]['description'].copy()\n",
    "temp_3 = lem_and_stem_3gram[ix_id]['description'].copy()\n",
    "temp_1.sort(), temp_2.sort(), temp_3.sort()\n",
    "print('--------------------------------------------------- 1 grams -------------------------------------------------')\n",
    "print(temp_1), \n",
    "print('\\n--------------------------------------------------- 2 grams -------------------------------------------------')\n",
    "print(temp_2)\n",
    "print('\\n--------------------------------------------------- 3 grams -------------------------------------------------')\n",
    "print(temp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the single words with the bigrams:\n",
    "By looking at the printouts above, the bigrams makes a lot of sense, e.g: real-world problem, social network linear algebra, topic model, etc. \n",
    "\n",
    "The trigrams on the other hand are not very informative. We skip the trigrams and continue with a corpus where we merged the 2-grams and 1-grams. \n",
    "\n",
    "Here on after we continue with the stemmed and lemmatized corpus containing the 1-grams and 2-grams. By looking at a few examples like the one above, the words do not seem to be overstemmed so we use stemming in combination with the lemmatizing.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not care for the position here so just add them up\n",
    "def corpus_merge(c1,c2):\n",
    "    merged_corpus =  copy.deepcopy(c1)\n",
    "    for i in range(len(c1)):\n",
    "        merged_corpus[i]['description'] = merged_corpus[i]['description'] + c2[i]['description']\n",
    "    return merged_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_stem_corpus = corpus_merge(lem_and_stem_1gram, lem_and_stem_2gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix\n",
    "\n",
    "Construct an M ×N term-document matrix X, where M is the number of terms and N is the\n",
    "number of documents. The matrix X should be sparse. You are not allowed to use libraries\n",
    "\n",
    "\n",
    "1. Print the 15 terms in the description of the IX class with the highest TF-IDF scores.\n",
    "2. Explain where the difference between the large scores and the small ones comes from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs a term-document matrix from a corpus\n",
    "def get_term_document_matrix(corpus):\n",
    "    global_dictionary, dictionary_mapping = get_dictionary(corpus)\n",
    "    df_index = dict((k,len(list(set(v)))) for k,v in dictionary_mapping.items()) # A dict where the key is the term and the value is in how many documents the term is present\n",
    "    unique_words = list(df_index.keys()) #The unique words\n",
    "    word_to_index = dict(zip(unique_words,(range(len(unique_words))))) # Mapping from word to index (That we use for the encoding)\n",
    "    index_to_word = dict((v,k) for k,v in word_to_index.items()) # Mapping back from index to word. \n",
    "\n",
    "\n",
    "    m = len(unique_words)\n",
    "    n = len(corpus)\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    columns = []\n",
    "\n",
    "    for i in range(n):\n",
    "        tokens = corpus[i]['description']\n",
    "        loc_word_count = len(tokens)\n",
    "        loc_counts = Counter(tokens)\n",
    "        unique_tokens = list(loc_counts.keys())\n",
    "\n",
    "        for token in unique_tokens:\n",
    "            tf = loc_counts[token]/loc_word_count\n",
    "            df = df_index[token]\n",
    "            idf = np.log(n/(df+1))\n",
    "\n",
    "            rows.append(word_to_index[token])\n",
    "            columns.append(i)\n",
    "            values.append(tf*idf)\n",
    "\n",
    "    return csr_matrix((values, (rows, columns)), shape=(m, n)), index_to_word, word_to_index\n",
    "\n",
    "def get_column_scores(mat, i,index_to_word, n=-1):\n",
    "    a = mat.getcol(i)\n",
    "    non_zero_rows = csr_matrix.nonzero(a)[0]\n",
    "    d = {}\n",
    "    for i in non_zero_rows:\n",
    "        d[index_to_word[i]] = a[i,0]\n",
    "    order_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    if n == -1:\n",
    "        return Counter(order_d).most_common()\n",
    "    else:\n",
    "        return Counter(order_d).most_common(n)\n",
    "    \n",
    "def get_row_scores(mat, word,word_to_index, n=-1,corpus=courses):\n",
    "    a = mat.getrow(word_to_index[word])\n",
    "    non_zero_cols = csr_matrix.nonzero(a)[1]\n",
    "    d = {}\n",
    "    for i in non_zero_cols:\n",
    "        d[corpus[i]['name']] = a[0,i]\n",
    "    \n",
    "    order_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    if n == -1:\n",
    "        return Counter(order_d).most_common()\n",
    "    else:\n",
    "        return Counter(order_d).most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('onlin', 0.07574768262684738),\n",
       " ('real-world', 0.0734207339706497),\n",
       " ('social', 0.06825318057153912),\n",
       " ('explor', 0.06605836788388599),\n",
       " (('data', 'mine'), 0.06523910163895413),\n",
       " (('social', 'network'), 0.06321080322017443),\n",
       " ('mine', 0.05990403442700461),\n",
       " ('large-scal', 0.05153576118068886),\n",
       " ('hadoop', 0.04957297285193386),\n",
       " (('system', 'cluster'), 0.04957297285193386),\n",
       " (('commun', 'detect'), 0.04957297285193386),\n",
       " ('e-commerc', 0.04704944590060245),\n",
       " (('recommend', 'system'), 0.04704944590060245),\n",
       " (('topic', 'model'), 0.04704944590060245),\n",
       " ('servic', 0.046998386361060844)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, index_to_word, word_to_index = get_term_document_matrix(lem_stem_corpus)\n",
    "ix_id = 43\n",
    "get_column_scores(X,ix_id,index_to_word,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('onlin', 5),\n",
       " ('real-world', 4),\n",
       " ('social', 5),\n",
       " ('explor', 5),\n",
       " (('data', 'mine'), 3),\n",
       " (('social', 'network'), 3),\n",
       " ('mine', 3),\n",
       " ('large-scal', 3),\n",
       " ('hadoop', 2),\n",
       " (('system', 'cluster'), 2),\n",
       " (('commun', 'detect'), 2),\n",
       " ('e-commerc', 2),\n",
       " (('recommend', 'system'), 2),\n",
       " (('topic', 'model'), 2),\n",
       " ('servic', 3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_id_scores = get_column_scores(X,ix_id,index_to_word,15)\n",
    "l = []\n",
    "for w in ix_id_scores:\n",
    "    l.append((w[0], lem_stem_corpus[ix_id]['description'].count(w[0])))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('onlin', 26),\n",
       " ('real-world', 12),\n",
       " ('social', 37),\n",
       " ('explor', 41),\n",
       " (('data', 'mine'), 5),\n",
       " (('social', 'network'), 6),\n",
       " ('mine', 8),\n",
       " ('large-scal', 16),\n",
       " ('hadoop', 2),\n",
       " (('system', 'cluster'), 2),\n",
       " (('commun', 'detect'), 2),\n",
       " ('e-commerc', 3),\n",
       " (('recommend', 'system'), 3),\n",
       " (('topic', 'model'), 3),\n",
       " ('servic', 23)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_dictionary, dictionary_mapping = get_dictionary(lem_stem_corpus)\n",
    "df_index = dict((k,len(list(set(v)))) for k,v in dictionary_mapping.items())\n",
    "l = []\n",
    "for w in ix_id_scores:\n",
    "    l.append((w[0], df_index[w[0]]))\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in TD-IDF Score comes from the fact that words with a higher TD-IDF score are either very frequent in description and/or occurs in fewer documents. We can see above how many times each lemmatized and stemmed word/bigram is present in the description of Internet Analytics and in how many documents they occur. As we can see the onlin, social and explor are present equally many times in the description however onlin has a a higher score. This is because onlin occurs less frequently and thus is a word that is less diluted (resulting in a higher IDF score). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search\n",
    "\n",
    "Search for \"markov chains\" and \"facebook\".\n",
    "\n",
    "1. Display the top five courses together with their similarity score for each query.\n",
    "2. What do you think of the results? Give your intuition on what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    return np.dot(np.transpose(a),b)/(norm(a)*norm(b))\n",
    "\n",
    "#Returns the n most similar courses to the search term. \n",
    "def term_query(term,n=5):\n",
    "    b = np.zeros((10766,1))\n",
    "    if term not in word_to_index:\n",
    "        print('Try another term, not present in the global dictionary')\n",
    "        return \n",
    "    term_index = word_to_index[term]\n",
    "    b[term_index] = 1\n",
    "    term_scores = {}\n",
    "    l = [0] * len(courses)\n",
    "    for i in range(len(courses)):\n",
    "        u = cosine_similarity(X.getcol(i).toarray(),b)[0][0]\n",
    "        if u > 0: #We're only interested on those courses with a higher similarity than 0\n",
    "            term_scores[i] = u\n",
    "    return dict((course_id_to_name[k],v) for k,v in dict(Counter(term_scores).most_common(n)).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- markov chains ------------------------\n",
      "Applied stochastic processes: 0.3453\n",
      "Markov chains and algorithmic applications: 0.2822\n",
      "Applied probability & stochastic processes: 0.2615\n",
      "Optimization and simulation: 0.1025\n",
      "Networks out of control: 0.0742\n",
      "\n",
      "---------------------- facebook ------------------------\n",
      "Computational Social Media: 0.1369\n"
     ]
    }
   ],
   "source": [
    "print('---------------------- markov chains ------------------------')\n",
    "markov_chain = term_query(('markov', 'chain'))\n",
    "for k,v in markov_chain.items():\n",
    "    print(k + ': '+ str(np.round(v,4)))\n",
    "print('\\n---------------------- facebook ------------------------')\n",
    "facebook = term_query('facebook')\n",
    "for k,v in facebook.items():\n",
    "    print(k + ': '+ str(np.round(v,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have one course with term 'facebook' in it, thus it is the only one we return when we run the query (all other courses will have a similarity score of 0). To see why this is the case we observe the numerator. It contains:\n",
    "\n",
    "1. $d_{i}^{T}$ - our encoding for our search term which contains zeros in all entries except for the entry corresponding to the term facebook. \n",
    "\n",
    "2. $d_{j}$ our encoding for document j, which only contains non-zeros elemnts for the words which are in the  desciption. \n",
    "\n",
    "Thereby $d_{i}^{T}d_{j}$ = 0, when document j does not have the term facebook in it. In order to find some courses that have content related to facebook we could run a query on the somewhat broader term social network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- social network ------------------------\n",
      "Internet analytics: 0.197\n",
      "Networks out of control: 0.0763\n",
      "Computational Social Media: 0.0724\n",
      "A Network Tour of Data Science: 0.0711\n",
      "Applied data analysis: 0.0441\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------------- social network ------------------------')\n",
    "social_network = term_query(('social', 'network'))\n",
    "for k,v in social_network.items():\n",
    "    print(k + ': '+ str(np.round(v,4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
