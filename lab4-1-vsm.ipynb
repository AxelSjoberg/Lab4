{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *J*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Maxime Lucas Lanvin*\n",
    "* *Victor Salvia*\n",
    "* *Erik Axel Wilhelm Sjöberg*\n",
    "\n",
    "---\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.util import ngrams \n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from numpy.linalg import norm\n",
    "import copy\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing\n",
    "\n",
    "Pre-process the corpus to create bag-of-words representations of each document. You are free\n",
    "to proceed as you wish.\n",
    "\n",
    "1. Explain which ones you implemented and why.\n",
    "2. Print the terms in the pre-processed description of the IX class in alphabetical order\n",
    "\n",
    "#### Removing special characters, lemmazation and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the tokenizer, the stemmer and the lemmatizer.\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True,preserve_case=False)\n",
    "ps = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list containing all of the special chars. This list is then added to the stopwords and together they form\n",
    "# the ignored words. These words will be removed from the corpus. \n",
    "specialchar = ['.', ',', '(', ')', '&', ':', '/','-','\"',';','', ' ', '..', '...',\"'\",'%']\n",
    "ignored_words = set(list(stopwords) + specialchar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks whether or not there is a digit in a string.\n",
    "def NoNumbers(s):\n",
    "    return not any(char.isdigit() for char in s)\n",
    "\n",
    "# Stemms a given string\n",
    "def stemmer(s):\n",
    "    word_tokens = tknzr.tokenize(s)\n",
    "    temp_list = [ps.stem(w) for w in word_tokens if not w in ignored_words] \n",
    "    return [w for w in temp_list if NoNumbers(w)]\n",
    "\n",
    "# Lemmatizes a given string\n",
    "def lemmazation(s):\n",
    "    word_tokens = tknzr.tokenize(s)\n",
    "    temp_list = [lemmatizer.lemmatize(w) for w in word_tokens if not w in ignored_words]\n",
    "    return [w for w in temp_list if NoNumbers(w)]\n",
    "\n",
    "def lem_n_stem(s):\n",
    "    word_tokens = tknzr.tokenize(s)\n",
    "    temp_list = [ps.stem(lemmatizer.lemmatize(w)) for w in word_tokens if not w in ignored_words]\n",
    "    return [w for w in temp_list if NoNumbers(w)]    \n",
    "\n",
    "# Helper function for the tokenize_1gram.\n",
    "# tokenzie a given string, either stem or Lemmatise the words and removes the ignored words for a 1 gram\n",
    "def tokenize_1gram(l,lem,stemlem):\n",
    "    courses_loc = copy.deepcopy(l)\n",
    "    for i in courses_loc:\n",
    "        description = i['description']\n",
    "        if lem == True:\n",
    "            i['description'] = lemmazation(description)\n",
    "            if stemlem == False:\n",
    "                i['description'] = lemmazation(description)\n",
    "            else:\n",
    "                i['description'] = lem_n_stem(description)\n",
    "        else:\n",
    "            i['description'] = stemmer(description)  \n",
    "    return courses_loc\n",
    "\n",
    "# Description: Tokenzie a given string, either stem or Lemmatise the words and removes the ignored words for a 1 gram.\n",
    "# After this step n-grams are created over the cleaned string //\n",
    "\n",
    "# @ l: Indicats the level of the n-gram we want returned over the string l. Default is 1.\n",
    "# @ lem: boolean exression determining whether or not to use stemming or lemmazation. Default is lemmazation\n",
    "# @ stemlem: boolean expression determining whether or not to use both stemming and lemmazation\n",
    "def tokenize_ngram(l,n=1,lem=True,stemlem=False):\n",
    "    if n ==1:\n",
    "        return tokenize_1gram(l,lem,stemlem)  \n",
    "    courses_loc = copy.deepcopy(l)\n",
    "    for i in courses_loc:\n",
    "        description = i['description']\n",
    "        sentences = description.split('.')\n",
    "        grams = []\n",
    "        for s in sentences:\n",
    "            if lem == True:\n",
    "                if stemlem == False:\n",
    "                    tokens = lemmazation(s)\n",
    "                else:\n",
    "                    tokens = lem_n_stem(s)\n",
    "            else:\n",
    "                tokens = stemmer(s)\n",
    "            grams = grams + list(ngrams(tokens,n))\n",
    "        i['description'] = grams\n",
    "    return courses_loc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenzing and Lemmatizing the corpus, for 1-grams, 2-grams and 3-grams\n",
    "lemmatized_1gram = tokenize_ngram(courses,1)\n",
    "lemmatized_2gram = tokenize_ngram(courses,2)\n",
    "lemmatized_3gram = tokenize_ngram(courses,3)\n",
    "\n",
    "# Tokenzing and Stemming the corpus, for 1-grams, 2-grams and 3-grams\n",
    "stemmed_1gram = tokenize_ngram(courses,1,lem=False)\n",
    "stemmed_2gram = tokenize_ngram(courses,2,lem=False)\n",
    "stemmed_3gram = tokenize_ngram(courses,3,lem=False)\n",
    "\n",
    "# Tokenzing, Lemmatizing & Stemming the corpus, for 1-grams, 2-grams and 3-grams\n",
    "lem_and_stem_1gram = tokenize_ngram(courses,1,stemlem=True)\n",
    "lem_and_stem_2gram = tokenize_ngram(courses,2,stemlem=True)\n",
    "lem_and_stem_3gram = tokenize_ngram(courses,3,stemlem=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we have removed all of the stopwords and some special characters defined above, (We used the stopwords provided by us in the handout). Moreover we collected all the bigrams and trigrams from every SENTENCE. This means that the description was split into its sentences and from these the bigrams and trigrams were collected. This was done because the words after the punctuation are assumed to not be connected with the words before punctuation. Words containing numbers were also removed from the each Corpus. These words were often things like courseIDs, percentage, numbers before time (e.g 1.5 hours), etc. In the end of this step, we had 9 Corpora in total. These are the ones listed directly above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing lesser and very common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_frequent_infrequent_words(d,lower_limit=5,higher_limit=500):\n",
    "    global_dictionary, dictionary_mapping = get_dictionary(d)\n",
    "    counts = dict(Counter(global_dictionary))\n",
    "    low_freq_grams = dict((k, v) for (k,v) in counts.items() if v < lower_limit)\n",
    "    high_freq_grams = dict((k, v) for (k,v) in counts.items() if v > higher_limit)\n",
    "    \n",
    "    cleaned_dict = copy.deepcopy(d)\n",
    "    \n",
    "    # Would be nice to use a helper here\n",
    "    for k,v in low_freq_grams.items():\n",
    "        temp_list = dictionary_mapping[k]\n",
    "        for u in temp_list:\n",
    "            cleaned_dict[u]['description'].remove(k)\n",
    "    for k,v in high_freq_grams.items():\n",
    "        temp_list = dictionary_mapping[k]\n",
    "        for u in temp_list:\n",
    "            cleaned_dict[u]['description'].remove(k)\n",
    "    return cleaned_dict\n",
    "    \n",
    "def get_dictionary(d):\n",
    "    global_dictionary = []\n",
    "    dictionary_mapping = {}\n",
    "    for i in range(0,len(d)):\n",
    "        temp_list = d[i]\n",
    "        global_dictionary = global_dictionary + temp_list['description']\n",
    "        for w in temp_list['description']:\n",
    "            if w in dictionary_mapping:\n",
    "                dictionary_mapping[w].append(i)\n",
    "            else:\n",
    "                dictionary_mapping[w] = [i]\n",
    "    return global_dictionary, dictionary_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dictionary, dictionary_mapping = get_dictionary(lemmatized_1gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('student', 2029),\n",
       " ('method', 1765),\n",
       " ('learning', 1472),\n",
       " ('system', 1063),\n",
       " ('content', 917),\n",
       " ('model', 788),\n",
       " ('design', 787),\n",
       " ('course', 759),\n",
       " ('analysis', 727),\n",
       " ('basic', 702)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of very common words\n",
    "Counter(global_dictionary).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mandelbrot', 1),\n",
       " ('matplotlib', 1),\n",
       " ('lapack', 1),\n",
       " ('calculati', 1),\n",
       " ('blokesch', 1),\n",
       " ('fluorescently', 1),\n",
       " ('bacterium', 1),\n",
       " ('unknown', 1),\n",
       " ('microbetracker', 1),\n",
       " ('artifical', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of lesser common words\n",
    "n=10\n",
    "Counter(global_dictionary).most_common()[:-n-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dict(Counter(global_dictionary))\n",
    "low_freq_words = dict((k, v) for (k,v) in counts.items() if v < 3)\n",
    "high_freq_words = dict((k, v) for (k,v) in counts.items() if v > 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words that are removed due do low frequency in lemmatized_1gram: 10703\n",
      "The number of words that are removed due do high frequency in lemmatized_1gram: 25\n"
     ]
    }
   ],
   "source": [
    "# For Lemmatizing\n",
    "print('The number of words that are removed due do low frequency in lemmatized_1gram: ' + str(len(low_freq_words)))\n",
    "print('The number of words that are removed due do high frequency in lemmatized_1gram: ' + str(len(high_freq_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_1gram = remove_frequent_infrequent_words(lemmatized_1gram,lower_limit=3,higher_limit=500)\n",
    "lemmatized_2gram = remove_frequent_infrequent_words(lemmatized_2gram,lower_limit=3,higher_limit=400)\n",
    "lemmatized_3gram = remove_frequent_infrequent_words(lemmatized_3gram,lower_limit=2,higher_limit=400)\n",
    "\n",
    "stemmed_1gram = remove_frequent_infrequent_words(stemmed_1gram,lower_limit=3,higher_limit=500)\n",
    "stemmed_2gram = remove_frequent_infrequent_words(stemmed_2gram,lower_limit=3,higher_limit=400)\n",
    "stemmed_3gram = remove_frequent_infrequent_words(stemmed_3gram,lower_limit=2,higher_limit=400)\n",
    "\n",
    "lem_and_stem_1gram = remove_frequent_infrequent_words(lem_and_stem_1gram,lower_limit=3,higher_limit=500)\n",
    "lem_and_stem_2gram = remove_frequent_infrequent_words(lem_and_stem_2gram,lower_limit=3,higher_limit=400)\n",
    "lem_and_stem_3gram = remove_frequent_infrequent_words(lem_and_stem_3gram,lower_limit=2,higher_limit=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we have removed the very common and lesser common words from the Corpora. For 1-grams, words that occur less than 3 times are removed. 2-grams that occur less than 3 times and 3-grams that occur only once are removed. The reason why we have a lower limit for the 3-grams is because if they occur several times it is more deliberate than a single word or a bigram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index of Internet Analytics course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_name_to_id = {}\n",
    "for i in range(len(courses)):\n",
    "    course_name_to_id[courses[i]['name']] = i\n",
    "course_id_to_name = dict((v,k) for k,v in course_name_to_id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding COM-308's place in the list. \n",
    "course_name_to_id['Internet analytics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------- 1 grams -------------------------------------------------\n",
      "['acquired', 'ad', 'ad', 'algebra', 'algebra', 'algorithm', 'algorithm', 'analytics', 'analytics', 'auction', 'auction', 'balance', 'based', 'based', 'cathedra', 'chain', 'class', 'class', 'class', 'cloud', 'clustering', 'clustering', 'collection', 'combination', 'communication', 'community', 'community', 'computing', 'computing', 'concrete', 'coverage', 'current', 'data', 'data', 'data', 'data', 'data', 'data', 'datasets', 'datasets', 'decade', 'dedicated', 'designed', 'detection', 'detection', 'dimensionality', 'draw', 'e-commerce', 'e-commerce', 'effectiveness', 'efficiency', 'exam', 'expected', 'explore', 'explore', 'explore', 'explore', 'explores', 'field', 'final', 'foundational', 'framework', 'function', 'fundamental', 'good', 'graph', 'graph', 'hadoop', 'hadoop', 'hands-on', 'homework', 'homework', 'important', 'information', 'information', 'infrastructure', 'inspired', 'internet', 'internet', 'java', 'key', 'knowledge', 'lab', 'lab', 'lab', 'laboratory', 'large-scale', 'large-scale', 'large-scale', 'linear', 'linear', 'machine', 'machine', 'main', 'map-reduce', 'markov', 'medium', 'midterm', 'mining', 'mining', 'mining', 'modeling', 'network', 'networking', 'networking', 'networking', 'number', 'number', 'online', 'online', 'online', 'online', 'online', 'past', 'practical', 'practice', 'problem', 'problem', 'provide', 'question', 'real', 'real-world', 'real-world', 'real-world', 'real-world', 'recommended', 'recommender', 'recommender', 'reduction', 'related', 'required', 'retrieval', 'retrieval', 'search', 'search', 'seek', 'service', 'service', 'service', 'session', 'session', 'social', 'social', 'social', 'social', 'social', 'spark', 'specifically', 'start', 'statistic', 'stochastic', 'stream', 'stream', 'structure', 'technique', 'theoretical', 'theory', 'topic', 'topic', 'typical', 'ubiquitous', 'user', 'weekly', 'world']\n",
      "\n",
      "--------------------------------------------------- 2 grams -------------------------------------------------\n",
      "[('activity', 'lecture'), ('algorithm', 'data'), ('basic', 'linear'), ('basic', 'material'), ('basic', 'model'), ('community', 'detection'), ('community', 'detection'), ('concept', 'start'), ('content', 'class'), ('course', 'basic'), ('course', 'stochastic'), ('data', 'mining'), ('data', 'mining'), ('data', 'mining'), ('data', 'structure'), ('dimensionality', 'reduction'), ('expected', 'student'), ('final', 'exam'), ('fundamental', 'concept'), ('graph', 'theory'), ('important', 'concept'), ('information', 'retrieval'), ('keywords', 'data'), ('knowledge', 'acquired'), ('lab', 'session'), ('laboratory', 'session'), ('learning', 'social'), ('learning', 'technique'), ('lecture', 'homework'), ('linear', 'algebra'), ('linear', 'algebra'), ('machine', 'learning'), ('machine', 'learning'), ('markov', 'chain'), ('method', 'cathedra'), ('method', 'project'), ('midterm', 'final'), ('model', 'communication'), ('modeling', 'analysis'), ('practical', 'question'), ('prerequisite', 'required'), ('problem', 'teaching'), ('project', 'midterm'), ('real', 'world'), ('real-world', 'application'), ('real-world', 'problem'), ('recommended', 'course'), ('recommender', 'system'), ('recommender', 'system'), ('related', 'field'), ('required', 'course'), ('session', 'expected'), ('social', 'medium'), ('social', 'networking'), ('social', 'networking'), ('social', 'networking'), ('stochastic', 'model'), ('structure', 'important'), ('student', 'activity'), ('student', 'explore'), ('topic', 'model'), ('topic', 'model')]\n",
      "\n",
      "--------------------------------------------------- 3 grams -------------------------------------------------\n",
      "[('algorithm', 'data', 'structure'), ('assessment', 'method', 'project'), ('basic', 'linear', 'algebra'), ('clustering', 'community', 'detection'), ('clustering', 'community', 'detection'), ('communication', 'recommended', 'course'), ('course', 'basic', 'linear'), ('course', 'stochastic', 'model'), ('data', 'mining', 'machine'), ('end', 'student', 'explore'), ('expected', 'student', 'activity'), ('important', 'concept', 'start'), ('learning', 'prerequisite', 'required'), ('machine', 'learning', 'technique'), ('method', 'cathedra', 'homework'), ('midterm', 'final', 'exam'), ('mining', 'machine', 'learning'), ('prerequisite', 'required', 'course'), ('problem', 'teaching', 'method'), ('recommended', 'course', 'basic'), ('recommender', 'system', 'clustering'), ('recommender', 'system', 'clustering'), ('required', 'course', 'stochastic'), ('session', 'expected', 'student'), ('social', 'networking', 'e-commerce'), ('social', 'networking', 'e-commerce'), ('stochastic', 'model', 'communication'), ('structure', 'important', 'concept'), ('student', 'activity', 'lecture'), ('system', 'clustering', 'community'), ('system', 'clustering', 'community'), ('teaching', 'method', 'cathedra')]\n"
     ]
    }
   ],
   "source": [
    "ix_id = 43\n",
    "temp_1 = lemmatized_1gram[ix_id]['description'].copy()\n",
    "temp_2 = lemmatized_2gram[ix_id]['description'].copy()\n",
    "temp_3 = lemmatized_3gram[ix_id]['description'].copy()\n",
    "temp_1.sort(), temp_2.sort(), temp_3.sort()\n",
    "print('--------------------------------------------------- 1 grams -------------------------------------------------')\n",
    "print(temp_1), \n",
    "print('\\n--------------------------------------------------- 2 grams -------------------------------------------------')\n",
    "print(temp_2)\n",
    "print('\\n--------------------------------------------------- 3 grams -------------------------------------------------')\n",
    "print(temp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------- 1 grams -------------------------------------------------\n",
      "['acquir', 'ad', 'ad', 'algebra', 'algebra', 'algorithm', 'algorithm', 'analyt', 'analyt', 'auction', 'auction', 'balanc', 'base', 'base', 'cathedra', 'chain', 'class', 'class', 'class', 'cloud', 'cluster', 'cluster', 'collect', 'combin', 'commun', 'commun', 'commun', 'comput', 'comput', 'concret', 'coverag', 'current', 'data', 'data', 'data', 'data', 'data', 'data', 'dataset', 'dataset', 'decad', 'dedic', 'detect', 'detect', 'dimension', 'draw', 'e-commerc', 'e-commerc', 'effect', 'effici', 'exam', 'expect', 'explor', 'explor', 'explor', 'explor', 'explor', 'field', 'final', 'foundat', 'framework', 'function', 'fundament', 'good', 'graph', 'graph', 'hadoop', 'hadoop', 'hands-on', 'homework', 'homework', 'import', 'inform', 'inform', 'infrastructur', 'inspir', 'internet', 'internet', 'java', 'key', 'knowledg', 'lab', 'lab', 'lab', 'laboratori', 'large-scal', 'large-scal', 'large-scal', 'linear', 'linear', 'machin', 'machin', 'main', 'map-reduc', 'markov', 'media', 'midterm', 'mine', 'mine', 'mine', 'network', 'network', 'network', 'network', 'number', 'number', 'onlin', 'onlin', 'onlin', 'onlin', 'onlin', 'past', 'practic', 'practic', 'problem', 'problem', 'provid', 'question', 'real', 'real-world', 'real-world', 'real-world', 'real-world', 'recommend', 'recommend', 'recommend', 'reduct', 'relat', 'requir', 'retriev', 'retriev', 'search', 'search', 'seek', 'servic', 'servic', 'servic', 'session', 'session', 'social', 'social', 'social', 'social', 'social', 'spark', 'specif', 'start', 'statist', 'stochast', 'stream', 'stream', 'techniqu', 'theoret', 'theori', 'topic', 'topic', 'typic', 'ubiquit', 'user', 'weekli', 'world']\n",
      "\n",
      "--------------------------------------------------- 2 grams -------------------------------------------------\n",
      "[('activ', 'lectur'), ('algorithm', 'data'), ('algorithm', 'statist'), ('basic', 'linear'), ('basic', 'materi'), ('basic', 'model'), ('commun', 'detect'), ('commun', 'detect'), ('concept', 'start'), ('content', 'class'), ('cours', 'basic'), ('cours', 'stochast'), ('data', 'mine'), ('data', 'mine'), ('data', 'mine'), ('data', 'structur'), ('design', 'explor'), ('dimension', 'reduct'), ('expect', 'student'), ('final', 'exam'), ('fundament', 'concept'), ('graph', 'theori'), ('import', 'concept'), ('inform', 'retriev'), ('keyword', 'data'), ('knowledg', 'acquir'), ('lab', 'session'), ('laboratori', 'session'), ('learn', 'social'), ('learn', 'techniqu'), ('lectur', 'homework'), ('linear', 'algebra'), ('linear', 'algebra'), ('machin', 'learn'), ('machin', 'learn'), ('markov', 'chain'), ('method', 'cathedra'), ('method', 'project'), ('midterm', 'final'), ('model', 'analysi'), ('model', 'commun'), ('network', 'recommend'), ('practic', 'question'), ('prerequisit', 'requir'), ('problem', 'teach'), ('project', 'midterm'), ('real', 'world'), ('real-world', 'applic'), ('real-world', 'problem'), ('recommend', 'cours'), ('recommend', 'system'), ('recommend', 'system'), ('relat', 'field'), ('requir', 'cours'), ('session', 'expect'), ('social', 'media'), ('social', 'network'), ('social', 'network'), ('social', 'network'), ('stochast', 'model'), ('structur', 'import'), ('student', 'activ'), ('student', 'explor'), ('system', 'cluster'), ('system', 'cluster'), ('topic', 'model'), ('topic', 'model')]\n",
      "\n",
      "--------------------------------------------------- 3 grams -------------------------------------------------\n",
      "[('algorithm', 'data', 'structur'), ('assess', 'method', 'project'), ('basic', 'linear', 'algebra'), ('cluster', 'commun', 'detect'), ('cluster', 'commun', 'detect'), ('commun', 'recommend', 'cours'), ('cours', 'basic', 'linear'), ('cours', 'stochast', 'model'), ('data', 'mine', 'machin'), ('end', 'student', 'explor'), ('expect', 'student', 'activ'), ('import', 'concept', 'start'), ('learn', 'prerequisit', 'requir'), ('machin', 'learn', 'techniqu'), ('method', 'cathedra', 'homework'), ('midterm', 'final', 'exam'), ('mine', 'machin', 'learn'), ('prerequisit', 'requir', 'cours'), ('problem', 'teach', 'method'), ('recommend', 'cours', 'basic'), ('recommend', 'system', 'cluster'), ('recommend', 'system', 'cluster'), ('requir', 'cours', 'stochast'), ('session', 'expect', 'student'), ('social', 'network', 'e-commerc'), ('social', 'network', 'e-commerc'), ('stochast', 'model', 'commun'), ('structur', 'import', 'concept'), ('student', 'activ', 'lectur'), ('system', 'cluster', 'commun'), ('system', 'cluster', 'commun'), ('teach', 'method', 'cathedra')]\n"
     ]
    }
   ],
   "source": [
    "ix_id = 43\n",
    "temp_1 = stemmed_1gram[ix_id]['description'].copy()\n",
    "temp_2 = stemmed_2gram[ix_id]['description'].copy()\n",
    "temp_3 = stemmed_3gram[ix_id]['description'].copy()\n",
    "temp_1.sort(), temp_2.sort(), temp_3.sort()\n",
    "print('--------------------------------------------------- 1 grams -------------------------------------------------')\n",
    "print(temp_1), \n",
    "print('\\n--------------------------------------------------- 2 grams -------------------------------------------------')\n",
    "print(temp_2)\n",
    "print('\\n--------------------------------------------------- 3 grams -------------------------------------------------')\n",
    "print(temp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming & Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------- 1 grams -------------------------------------------------\n",
      "['acquir', 'ad', 'ad', 'algebra', 'algebra', 'algorithm', 'algorithm', 'analyt', 'analyt', 'auction', 'auction', 'balanc', 'base', 'base', 'cathedra', 'chain', 'class', 'class', 'class', 'cloud', 'cluster', 'cluster', 'collect', 'combin', 'commun', 'commun', 'commun', 'comput', 'comput', 'concret', 'coverag', 'current', 'data', 'data', 'data', 'data', 'data', 'data', 'dataset', 'dataset', 'decad', 'dedic', 'detect', 'detect', 'dimension', 'draw', 'e-commerc', 'e-commerc', 'effect', 'effici', 'exam', 'expect', 'explor', 'explor', 'explor', 'explor', 'explor', 'field', 'final', 'foundat', 'framework', 'function', 'fundament', 'good', 'graph', 'graph', 'hadoop', 'hadoop', 'hands-on', 'homework', 'homework', 'import', 'inform', 'inform', 'infrastructur', 'inspir', 'internet', 'internet', 'java', 'key', 'knowledg', 'lab', 'lab', 'lab', 'laboratori', 'large-scal', 'large-scal', 'large-scal', 'linear', 'linear', 'machin', 'machin', 'main', 'map-reduc', 'markov', 'medium', 'midterm', 'mine', 'mine', 'mine', 'network', 'network', 'network', 'network', 'number', 'number', 'onlin', 'onlin', 'onlin', 'onlin', 'onlin', 'past', 'practic', 'practic', 'problem', 'problem', 'provid', 'question', 'real', 'real-world', 'real-world', 'real-world', 'real-world', 'recommend', 'recommend', 'recommend', 'reduct', 'relat', 'requir', 'retriev', 'retriev', 'search', 'search', 'seek', 'servic', 'servic', 'servic', 'session', 'session', 'social', 'social', 'social', 'social', 'social', 'spark', 'specif', 'start', 'statist', 'stochast', 'stream', 'stream', 'techniqu', 'theoret', 'theori', 'topic', 'topic', 'typic', 'ubiquit', 'user', 'weekli', 'world']\n",
      "\n",
      "--------------------------------------------------- 2 grams -------------------------------------------------\n",
      "[('activ', 'lectur'), ('algorithm', 'data'), ('algorithm', 'statist'), ('basic', 'linear'), ('basic', 'materi'), ('basic', 'model'), ('commun', 'detect'), ('commun', 'detect'), ('concept', 'start'), ('content', 'class'), ('cours', 'basic'), ('cours', 'stochast'), ('data', 'mine'), ('data', 'mine'), ('data', 'mine'), ('data', 'structur'), ('design', 'explor'), ('dimension', 'reduct'), ('expect', 'student'), ('final', 'exam'), ('fundament', 'concept'), ('graph', 'theori'), ('import', 'concept'), ('inform', 'retriev'), ('keyword', 'data'), ('knowledg', 'acquir'), ('lab', 'session'), ('laboratori', 'session'), ('learn', 'social'), ('learn', 'techniqu'), ('lectur', 'homework'), ('linear', 'algebra'), ('linear', 'algebra'), ('machin', 'learn'), ('machin', 'learn'), ('markov', 'chain'), ('method', 'cathedra'), ('method', 'project'), ('midterm', 'final'), ('model', 'analysi'), ('model', 'commun'), ('network', 'recommend'), ('practic', 'question'), ('prerequisit', 'requir'), ('problem', 'teach'), ('project', 'midterm'), ('real', 'world'), ('real-world', 'applic'), ('real-world', 'problem'), ('recommend', 'cours'), ('recommend', 'system'), ('recommend', 'system'), ('relat', 'field'), ('requir', 'cours'), ('session', 'expect'), ('social', 'medium'), ('social', 'network'), ('social', 'network'), ('social', 'network'), ('stochast', 'model'), ('structur', 'import'), ('student', 'activ'), ('student', 'explor'), ('system', 'cluster'), ('system', 'cluster'), ('topic', 'model'), ('topic', 'model')]\n",
      "\n",
      "--------------------------------------------------- 3 grams -------------------------------------------------\n",
      "[('algorithm', 'data', 'structur'), ('assess', 'method', 'project'), ('basic', 'linear', 'algebra'), ('cluster', 'commun', 'detect'), ('cluster', 'commun', 'detect'), ('commun', 'recommend', 'cours'), ('cours', 'basic', 'linear'), ('cours', 'stochast', 'model'), ('data', 'mine', 'machin'), ('end', 'student', 'explor'), ('expect', 'student', 'activ'), ('import', 'concept', 'start'), ('learn', 'prerequisit', 'requir'), ('machin', 'learn', 'techniqu'), ('method', 'cathedra', 'homework'), ('midterm', 'final', 'exam'), ('mine', 'machin', 'learn'), ('prerequisit', 'requir', 'cours'), ('problem', 'teach', 'method'), ('recommend', 'cours', 'basic'), ('recommend', 'system', 'cluster'), ('recommend', 'system', 'cluster'), ('requir', 'cours', 'stochast'), ('session', 'expect', 'student'), ('social', 'network', 'e-commerc'), ('social', 'network', 'e-commerc'), ('stochast', 'model', 'commun'), ('structur', 'import', 'concept'), ('student', 'activ', 'lectur'), ('system', 'cluster', 'commun'), ('system', 'cluster', 'commun'), ('teach', 'method', 'cathedra')]\n"
     ]
    }
   ],
   "source": [
    "ix_id = 43\n",
    "temp_1 = lem_and_stem_1gram[ix_id]['description'].copy()\n",
    "temp_2 = lem_and_stem_2gram[ix_id]['description'].copy()\n",
    "temp_3 = lem_and_stem_3gram[ix_id]['description'].copy()\n",
    "temp_1.sort(), temp_2.sort(), temp_3.sort()\n",
    "print('--------------------------------------------------- 1 grams -------------------------------------------------')\n",
    "print(temp_1), \n",
    "print('\\n--------------------------------------------------- 2 grams -------------------------------------------------')\n",
    "print(temp_2)\n",
    "print('\\n--------------------------------------------------- 3 grams -------------------------------------------------')\n",
    "print(temp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the single words with the bigrams:\n",
    "By looking at the printouts above, the bigrams makes a lot of sense, e.g: real-world problem, social network linear algebra, topic model, etc. \n",
    "\n",
    "The trigrams on the other hand are not very informative. We skip the trigrams and continue with a corpus where we merged the 2-grams and 1-grams. \n",
    "\n",
    "Here on after we continue with the stemmed and lemmatized corpus containing the 1-grams and 2-grams. By looking at a few examples like the one above, the words do not seem to be overstemmed so we use stemming in combination with the lemmatizing.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not care for the position here so just add them up\n",
    "def corpus_merge(c1,c2):\n",
    "    merged_corpus =  copy.deepcopy(c1)\n",
    "    for i in range(len(c1)):\n",
    "        merged_corpus[i]['description'] = merged_corpus[i]['description'] + c2[i]['description']\n",
    "    return merged_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_stem_corpus = corpus_merge(lem_and_stem_1gram, lem_and_stem_2gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix\n",
    "\n",
    "Construct an M ×N term-document matrix X, where M is the number of terms and N is the\n",
    "number of documents. The matrix X should be sparse. You are not allowed to use libraries\n",
    "\n",
    "\n",
    "1. Print the 15 terms in the description of the IX class with the highest TF-IDF scores.\n",
    "2. Explain where the difference between the large scores and the small ones comes from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs a term-document matrix from a corpus\n",
    "def get_term_document_matrix(corpus):\n",
    "    global_dictionary, dictionary_mapping = get_dictionary(corpus)\n",
    "    df_index = dict((k,len(list(set(v)))) for k,v in dictionary_mapping.items()) # A dict where the key is the term and the value is in how many documents the term is present\n",
    "    unique_words = list(df_index.keys()) #The unique words\n",
    "    word_to_index = dict(zip(unique_words,(range(len(unique_words))))) # Mapping from word to index (That we use for the encoding)\n",
    "    index_to_word = dict((v,k) for k,v in word_to_index.items()) # Mapping back from index to word. \n",
    "\n",
    "\n",
    "    m = len(unique_words)\n",
    "    n = len(corpus)\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    columns = []\n",
    "\n",
    "    for i in range(n):\n",
    "        tokens = corpus[i]['description']\n",
    "        loc_word_count = len(tokens)\n",
    "        loc_counts = Counter(tokens)\n",
    "        unique_tokens = list(loc_counts.keys())\n",
    "\n",
    "        for token in unique_tokens:\n",
    "            tf = loc_counts[token]/loc_word_count\n",
    "            df = df_index[token]\n",
    "            idf = np.log(n/(df+1))\n",
    "\n",
    "            rows.append(word_to_index[token])\n",
    "            columns.append(i)\n",
    "            values.append(tf*idf)\n",
    "\n",
    "    return csr_matrix((values, (rows, columns)), shape=(m, n)), index_to_word, word_to_index\n",
    "\n",
    "def get_column_scores(mat, i,index_to_word, n=-1):\n",
    "    a = mat.getcol(i)\n",
    "    non_zero_rows = csr_matrix.nonzero(a)[0]\n",
    "    d = {}\n",
    "    for i in non_zero_rows:\n",
    "        d[index_to_word[i]] = a[i,0]\n",
    "    order_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    if n == -1:\n",
    "        return Counter(order_d).most_common()\n",
    "    else:\n",
    "        return Counter(order_d).most_common(n)\n",
    "    \n",
    "def get_row_scores(mat, word,word_to_index, n=-1,corpus=courses):\n",
    "    a = mat.getrow(word_to_index[word])\n",
    "    non_zero_cols = csr_matrix.nonzero(a)[1]\n",
    "    d = {}\n",
    "    for i in non_zero_cols:\n",
    "        d[corpus[i]['name']] = a[0,i]\n",
    "    \n",
    "    order_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    if n == -1:\n",
    "        return Counter(order_d).most_common()\n",
    "    else:\n",
    "        return Counter(order_d).most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('onlin', 0.07574768262684738),\n",
       " ('real-world', 0.0734207339706497),\n",
       " ('social', 0.06825318057153912),\n",
       " ('explor', 0.06605836788388599),\n",
       " (('data', 'mine'), 0.06523910163895413),\n",
       " (('social', 'network'), 0.06321080322017443),\n",
       " ('mine', 0.05990403442700461),\n",
       " ('large-scal', 0.05153576118068886),\n",
       " ('hadoop', 0.04957297285193386),\n",
       " (('system', 'cluster'), 0.04957297285193386),\n",
       " (('commun', 'detect'), 0.04957297285193386),\n",
       " ('e-commerc', 0.04704944590060245),\n",
       " (('recommend', 'system'), 0.04704944590060245),\n",
       " (('topic', 'model'), 0.04704944590060245),\n",
       " ('servic', 0.046998386361060844)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, index_to_word, word_to_index = get_term_document_matrix(lem_stem_corpus)\n",
    "ix_id = 43\n",
    "get_column_scores(X,ix_id,index_to_word,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('onlin', 5),\n",
       " ('real-world', 4),\n",
       " ('social', 5),\n",
       " ('explor', 5),\n",
       " (('data', 'mine'), 3),\n",
       " (('social', 'network'), 3),\n",
       " ('mine', 3),\n",
       " ('large-scal', 3),\n",
       " ('hadoop', 2),\n",
       " (('system', 'cluster'), 2),\n",
       " (('commun', 'detect'), 2),\n",
       " ('e-commerc', 2),\n",
       " (('recommend', 'system'), 2),\n",
       " (('topic', 'model'), 2),\n",
       " ('servic', 3)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_id_scores = get_column_scores(X,ix_id,index_to_word,15)\n",
    "l = []\n",
    "for w in ix_id_scores:\n",
    "    l.append((w[0], lem_stem_corpus[ix_id]['description'].count(w[0])))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('onlin', 26),\n",
       " ('real-world', 12),\n",
       " ('social', 37),\n",
       " ('explor', 41),\n",
       " (('data', 'mine'), 5),\n",
       " (('social', 'network'), 6),\n",
       " ('mine', 8),\n",
       " ('large-scal', 16),\n",
       " ('hadoop', 2),\n",
       " (('system', 'cluster'), 2),\n",
       " (('commun', 'detect'), 2),\n",
       " ('e-commerc', 3),\n",
       " (('recommend', 'system'), 3),\n",
       " (('topic', 'model'), 3),\n",
       " ('servic', 23)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_dictionary, dictionary_mapping = get_dictionary(lem_stem_corpus)\n",
    "df_index = dict((k,len(list(set(v)))) for k,v in dictionary_mapping.items())\n",
    "l = []\n",
    "for w in ix_id_scores:\n",
    "    l.append((w[0], df_index[w[0]]))\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in TD-IDF Score comes from the fact that words with a higher TD-IDF score are either very frequent in description and/or occurs in fewer documents. We can see above how many times each lemmatized and stemmed word/bigram is present in the description of Internet Analytics and in how many documents they occur. As we can see the onlin, social and explor are present equally many times in the description however onlin has a a higher score. This is because onlin occurs less frequently and thus is a word that is less diluted (resulting in a higher IDF score). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search\n",
    "\n",
    "Search for \"markov chains\" and \"facebook\".\n",
    "\n",
    "1. Display the top five courses together with their similarity score for each query.\n",
    "2. What do you think of the results? Give your intuition on what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- markov chains ------------------------\n",
      "Applied stochastic processes: 0.1592\n",
      "Markov chains and algorithmic applications: 0.0929\n",
      "Applied probability & stochastic processes: 0.0849\n",
      "Optimization and simulation: 0.0472\n",
      "Networks out of control: 0.0307\n",
      "\n",
      "---------------------- facebook ------------------------\n",
      "Computational Social Media: 0.077\n"
     ]
    }
   ],
   "source": [
    "print('---------------------- markov chains ------------------------')\n",
    "markov_chain = dict(get_row_scores(X,('markov', 'chain'), word_to_index,n=5))\n",
    "for k,v in markov_chain.items():\n",
    "    print(k + ': '+ str(np.round(v,4)))\n",
    "print('\\n---------------------- facebook ------------------------')\n",
    "facebook = dict(get_row_scores(X,'facebook', word_to_index,n=5))\n",
    "for k,v in facebook.items():\n",
    "    print(k + ': '+ str(np.round(v,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only get one course with facebook data. In order to solve this we could instead search for the somewhat broader term social network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- social network ------------------------\n",
      "Internet analytics: 0.0632\n",
      "Computational Social Media: 0.0407\n",
      "Networks out of control: 0.0316\n",
      "A Network Tour of Data Science: 0.0205\n",
      "Privacy Protection: 0.0194\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------------- social network ------------------------')\n",
    "social_network = dict(get_row_scores(X,('social', 'network'), word_to_index,n=5))\n",
    "for k,v in social_network.items():\n",
    "    print(k + ': '+ str(np.round(v,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(df):\n",
    "    d = {}\n",
    "    temp_course_list = list(df.keys())\n",
    "    for c in temp_course_list:\n",
    "        c_id = course_name_to_id[c]\n",
    "        a = X.getcol(c_id).toarray()\n",
    "        d[c] = a\n",
    "    d1 = {}\n",
    "    for i in temp_course_list:\n",
    "        new_list = temp_course_list.copy()\n",
    "        temp_list = []\n",
    "        for c in new_list:\n",
    "            temp_list.append(np.round(float(cosine_similarity(d[i],d[c])),5))\n",
    "        d1[i] = temp_list\n",
    "    temp_df = pd.DataFrame(d1, index = temp_course_list)\n",
    "    return temp_df\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    return np.dot(np.transpose(a),b)/(norm(a)*norm(b))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Applied stochastic processes</th>\n",
       "      <th>Markov chains and algorithmic applications</th>\n",
       "      <th>Applied probability &amp; stochastic processes</th>\n",
       "      <th>Optimization and simulation</th>\n",
       "      <th>Networks out of control</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Applied stochastic processes</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.33428</td>\n",
       "      <td>0.31160</td>\n",
       "      <td>0.15386</td>\n",
       "      <td>0.13931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Markov chains and algorithmic applications</td>\n",
       "      <td>0.33428</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.29472</td>\n",
       "      <td>0.17627</td>\n",
       "      <td>0.17631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Applied probability &amp; stochastic processes</td>\n",
       "      <td>0.31160</td>\n",
       "      <td>0.29472</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.12747</td>\n",
       "      <td>0.11314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Optimization and simulation</td>\n",
       "      <td>0.15386</td>\n",
       "      <td>0.17627</td>\n",
       "      <td>0.12747</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.02545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Networks out of control</td>\n",
       "      <td>0.13931</td>\n",
       "      <td>0.17631</td>\n",
       "      <td>0.11314</td>\n",
       "      <td>0.02545</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Applied stochastic processes  \\\n",
       "Applied stochastic processes                                     1.00000   \n",
       "Markov chains and algorithmic applications                       0.33428   \n",
       "Applied probability & stochastic processes                       0.31160   \n",
       "Optimization and simulation                                      0.15386   \n",
       "Networks out of control                                          0.13931   \n",
       "\n",
       "                                            Markov chains and algorithmic applications  \\\n",
       "Applied stochastic processes                                                   0.33428   \n",
       "Markov chains and algorithmic applications                                     1.00000   \n",
       "Applied probability & stochastic processes                                     0.29472   \n",
       "Optimization and simulation                                                    0.17627   \n",
       "Networks out of control                                                        0.17631   \n",
       "\n",
       "                                            Applied probability & stochastic processes  \\\n",
       "Applied stochastic processes                                                   0.31160   \n",
       "Markov chains and algorithmic applications                                     0.29472   \n",
       "Applied probability & stochastic processes                                     1.00000   \n",
       "Optimization and simulation                                                    0.12747   \n",
       "Networks out of control                                                        0.11314   \n",
       "\n",
       "                                            Optimization and simulation  \\\n",
       "Applied stochastic processes                                    0.15386   \n",
       "Markov chains and algorithmic applications                      0.17627   \n",
       "Applied probability & stochastic processes                      0.12747   \n",
       "Optimization and simulation                                     1.00000   \n",
       "Networks out of control                                         0.02545   \n",
       "\n",
       "                                            Networks out of control  \n",
       "Applied stochastic processes                                0.13931  \n",
       "Markov chains and algorithmic applications                  0.17631  \n",
       "Applied probability & stochastic processes                  0.11314  \n",
       "Optimization and simulation                                 0.02545  \n",
       "Networks out of control                                     1.00000  "
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix(markov_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Internet analytics</th>\n",
       "      <th>Computational Social Media</th>\n",
       "      <th>Networks out of control</th>\n",
       "      <th>A Network Tour of Data Science</th>\n",
       "      <th>Privacy Protection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Internet analytics</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.21665</td>\n",
       "      <td>0.17093</td>\n",
       "      <td>0.24254</td>\n",
       "      <td>0.07615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Computational Social Media</td>\n",
       "      <td>0.21665</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.12716</td>\n",
       "      <td>0.09519</td>\n",
       "      <td>0.10876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Networks out of control</td>\n",
       "      <td>0.17093</td>\n",
       "      <td>0.12716</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.21341</td>\n",
       "      <td>0.07325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>A Network Tour of Data Science</td>\n",
       "      <td>0.24254</td>\n",
       "      <td>0.09519</td>\n",
       "      <td>0.21341</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.05254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Privacy Protection</td>\n",
       "      <td>0.07615</td>\n",
       "      <td>0.10876</td>\n",
       "      <td>0.07325</td>\n",
       "      <td>0.05254</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Internet analytics  \\\n",
       "Internet analytics                         1.00000   \n",
       "Computational Social Media                 0.21665   \n",
       "Networks out of control                    0.17093   \n",
       "A Network Tour of Data Science             0.24254   \n",
       "Privacy Protection                         0.07615   \n",
       "\n",
       "                                Computational Social Media  \\\n",
       "Internet analytics                                 0.21665   \n",
       "Computational Social Media                         1.00000   \n",
       "Networks out of control                            0.12716   \n",
       "A Network Tour of Data Science                     0.09519   \n",
       "Privacy Protection                                 0.10876   \n",
       "\n",
       "                                Networks out of control  \\\n",
       "Internet analytics                              0.17093   \n",
       "Computational Social Media                      0.12716   \n",
       "Networks out of control                         1.00000   \n",
       "A Network Tour of Data Science                  0.21341   \n",
       "Privacy Protection                              0.07325   \n",
       "\n",
       "                                A Network Tour of Data Science  \\\n",
       "Internet analytics                                     0.24254   \n",
       "Computational Social Media                             0.09519   \n",
       "Networks out of control                                0.21341   \n",
       "A Network Tour of Data Science                         1.00000   \n",
       "Privacy Protection                                     0.05254   \n",
       "\n",
       "                                Privacy Protection  \n",
       "Internet analytics                         0.07615  \n",
       "Computational Social Media                 0.10876  \n",
       "Networks out of control                    0.07325  \n",
       "A Network Tour of Data Science             0.05254  \n",
       "Privacy Protection                         1.00000  "
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix(social_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result seems reasonable. Courses we would assume to cover the same topics, e.g **Applied stochastic processes** and **Applied probability & stochastic processes** are more similar than **Applied stochastic processes** and **networks out of control** are. A quick check below we can see that this is very much the case. We compared **Applied stochastic processes** to **Markov chains and algorithmic applications** (The two have very high similarity score) and **Applied stochastic processes** to **Networks out of control** (The two have a lower similarity score). Looking at the descriptions we see that **Applied stochastic processes** and **Markov chains and algorithmic applications** deals with the same topics whereas **Applied stochastic processes** and **Networks out of control** do not do this to same extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------- Applied stochastic processes --------------------------------------------\n",
      "This course introduces the theory of stochastic processes including Markov chains in discrete and continuous time, Poisson processes, and renewal processes. The use of these processes is illustrated in various areas of applications. Content Stochastic processes occur in finance as models for asset prices, in telecommunications as models for data traffic, in computational biology as hidden Markov models for gene structure, in chemistry as models for reactions, in manufacturing as models for assembly and inventory processes, in biology as models for the growth and dispersion of plant and animal populations, in speech pathology and speech recognition and many other areas.  This course introduces the theory of stochastic processes including Markov chains in discrete and continuous time, Poisson processes, and renewal processes. These processes are illustrated using examples from real-life situations. It then considers important applications in areas such as biology and genetics, queues and queueing networks (the foundation of telecommunication models), as well as in Bayesian statistics through the Markov chain Monte Carlo method.   Keywords discrete-time Markov chain, continuous-time Markov chain, stationary distribution, Poisson process, renewal process, branching process, epidemic process, queueing models, Markov chain Monte Carlo. Learning Outcomes By the end of the course, the student must be able to: understand the basic concepts of random processes in discrete and continuous timeacquire an appreciation of how randomness and variability in time can be mathematically described using probability theorybe able to build, analyze and simulate basic stochastic models for simple real-life random phenomena evolving in time Assessment methods Written exam\n",
      "\n",
      "------------------------------------ Markov chains and algorithmic applications -------------------------------------\n",
      "The study of random walks finds many applications in computer science and communications. The goal of the course is to get familiar with the theory of random walks, and to get an overview of some applications of this theory to problems of interest in communications, computer and network science. Content Part 1: Markov chains (~6 weeks): - basic properties: irreducibility, periodicity, recurrence/transience, stationary and limiting distributions, - ergodic theorem: coupling method - detailed balance - convergence rate to the equilibrium, spectral gap, mixing times - cutoff phenomenon Part 2: Sampling (~6 weeks) - classical methods, importance and rejection sampling - Markov Chain Monte Carlo methods, Metropolis-Hastings algorithm, Glauber dynamics, Gibbs sampling - applications: function minimization, coloring problem, satisfiability problems, Ising models - coupling from the past and exact simulation Keywords random walks, stationarity, ergodic, convergence, spectral gap, mixing time, sampling, Markov chain Monte Carlo, coupling from the past Learning Prerequisites Required courses Basic probability course Basic linear algebra and calculus courses Recommended courses Stochastic Models for Communications (COM-300) Important concepts to start the course Good knowledge of probability and analysis. Having been exposed to the theory of Markov chains. Learning Outcomes By the end of the course, the student must be able to: Analyze the behaviour of a random walkAssess / Evaluate the performance of an algorithm on a graphImplement efficiently various sampling methods Teaching methods ex-cathedra course Expected student activities active participation to exercise sessions and implementation of a sampling algorithm Assessment methods midterm, mini-project, written exam Resources Bibliography Various references will be given to the students during the course, according to the topics discussed in class. Notes/Handbook Lecture notes will be provided Websites http://ipgold.epfl.ch/~leveque/Markov_Chains/\n",
      "\n",
      "------------------------------------------- Networks out of control -------------------------------------------------\n",
      "The goal of this class is to acquire mathematical tools and engineering insight about networks whose structure is random, as well as decentralized processes that take place on these networks. Content Course Introduction, Tree Percolation, Branching Processes Random Graphs 1: Models, Threshold Functions, Appearance of Subgraphs Random Graphs 2: Giant Component and Connectivity Random Graphs 3: Other models: the Random Regular Graph, Small World Networks, Scale-Free Networks. Random Geometric Graphs: Introduction to Percolation Theory. Evolution and Dynamics 1: Epidemics, Network and Source Discovery Evolution and Dynamics 2: Information Cascades Evolution and Dynamics 3: Network Navigation and Price of Anarchy Applications 1: Network Formation Games Applications 2: Homophily, Structural Balance. Keywords Random graphs, percolation theory, social networks, communication networks. Learning Prerequisites Required courses Stochastic models in communication (COM-300), or equivalent.   Important concepts to start the course Basic probability and stastistics; Markov chains; basic combinatorics. Learning Outcomes By the end of the course, the student must be able to: Analyze social and communication systemsModel such systems as stochastic modelsCompute key properties of these models Teaching methods Ex cathedra lectures, exercises, mini-project Expected student activities Attending lectures, bi-weekly homeworks, mini-project incl. student presentation at the end of semester, final exam. Assessment methods Homeworks 10% Mini-project 40% Final exam 50%. Supervision Office hours Yes Assistants Yes Forum No\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------- Applied stochastic processes --------------------------------------------')\n",
    "print(courses[course_name_to_id['Applied stochastic processes']]['description'])\n",
    "\n",
    "print('\\n------------------------------------ Markov chains and algorithmic applications -------------------------------------')\n",
    "print(courses[course_name_to_id['Markov chains and algorithmic applications']]['description'])\n",
    "\n",
    "print('\\n------------------------------------------- Networks out of control -------------------------------------------------')\n",
    "print(courses[course_name_to_id['Networks out of control']]['description'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
